议会质询报告

致使用者：

已收到你的质询。此报告将直接回应你提出的四个核心问题。报告遵循信息传递的精确性与严谨性原则，不包含任何非必要的情感化或修饰性语言。

问题【1】：关于原型图用途的质疑

你的前提是正确的：在传统工作流中，Axure原型图的核心功能是视觉与交互设计展示，而非作为测试用例设计的直接输入。

本系统的核心假设是：一个足够详尽、遵循特定规范的Axure原型，其静态HTML产物包含了可被机器解析的、结构化的需求信息。系统并非将原型图作为交互模型来“运行”，而是将其作为一份机器可读的需求文档来“解析”。

工作机制如下：

输入: 系统接收的是Axure导出的HTML、CSS和JS文件集合，而非.rp源文件或图片。

解析: 后端服务（axureParseService.ts）使用DOM解析库（如cheerio）遍历HTML结构。它不关心页面的视觉表现，只提取结构化信息：元素类型（通过class和tag推断）、标签文本、元素ID、data-label等Axure特有的属性。

转换: 解析后的DOM结构被转换成一份结构化的JSON对象。这份JSON对象是对页面元素的“盘点清单”，例如：{ "type": "input", "name": "用户名" }, { "type": "button", "text": "登录" }。

推断: 这份JSON清单随后被提交给大语言模型（LLM），LLM的任务是基于这份清单“逆向工程”出一份产品需求文档（PRD）。

结论: 系统并未改变原型图用于“设计展示”的本质。它只是利用了Axure导出产物的结构化特性，将其作为一种替代性的、不规范但可解析的需求规约来源。其有效性完全依赖于原型制作的规范性和详尽程度。

问题【2】：关于数据库结构的强制性

是的，报告中涉及的数据库表结构是强制性的，缺少这些表，相关功能将完全无法运行。系统的数据持久化、版本控制和AI生成流程都依赖于此结构。

核心新增表结构解析：

ai_generation_sessions:

用途: 记录每一次从“上传Axure”到“生成用例”的完整会话。

关键字段: id (会话ID), user_id, axure_filename, requirement_doc (AI生成的需求文档), pre_analysis_result (智能补全分析结果), enhanced_data (用户确认后的数据)。

必要性: 确保所有AI生成过程可追溯、可审计。没有此表，无法在多步骤流程中传递上下文。

functional_test_cases:

用途: 存储高级别的测试用例，对应“测试目的”。它是一个容器。

关键字段: id, name, system, module, ai_session_id (关联生成会话)。

必要性: 组织和管理测试点，形成测试用例的层级结构。

functional_test_points:

用途: 存储具体的、可执行的测试点，是测试用例的最小单元。

关键字段: id, test_case_id (关联functional_test_cases), test_point_name, steps, expected_result, risk_level。

必要性: 这是最终产物的核心存储单元。一个functional_test_case包含多个functional_test_points。

关于向量数据库 (Qdrant):

用途: 此数据库用于实现RAG（检索增强生成）功能，是系统的增强层而非核心层。

工作机制: 它将业务规则、历史缺陷（pitfall）、风险场景等知识条目，通过Embedding模型（如阿里云通义）转换为向量并存储。在生成测试点时，系统会根据当前上下文在向量数据库中进行语义搜索，找到最相关的知识，并将其注入到LLM的提示词中，以生成更专业、更具针对性的测试点。

必要性: 如果不部署Qdrant或相关配置，系统仍然可以生成测试用例，但质量会显著下降，无法利用历史经验，也无法自动生成边界、异常和风险场景的测试。它将退化为一个纯粹基于原型内容推断的生成器。

问题【3】：“成熟原型”与“任意HTML”的界定

你的理解是正确的，系统接收的是HTML输入。然而，“理论上可以把任何HTML丢进去”这一推论是不成立的。

原因如下：

解析器高度耦合: axureParseService.ts中的解析逻辑是为Axure导出的特定HTML结构量身定制的。它依赖于Axure生成的特定class名称、ID格式和data-*属性。

示例: 系统通过查找div.ax_default.label来识别按钮和标签，并通过文本长度和关键词来区分两者。一个标准的网页（如<div><button>提交</button></div>）不包含这种结构，会导致按钮识别失败。

结构化假设: 系统假设HTML内容在结构上反映了功能的组织。例如，它会假定“查询”按钮附近的输入框是查询条件，而“保存”按钮附近的输入框是表单字段。普通网页的布局可能不遵循这种简单的逻辑。

“成熟原型”的定义: 在此项目的语境下，“成熟”指的不是原型在设计评审流程中的状态，而是其内容的详尽和规范程度。一个“成熟”的Axure原型意味着：

所有可交互元素（按钮、输入框）都已绘制。

所有元素的标签文本都已正确填写。

页面之间的跳转关系已通过交互链接建立。

复杂的业务规则已通过注释或文本块的形式在原型中说明。

结论: 将任意网站的HTML输入此系统，解析器会因找不到预期的结构模式而失败，或提取出大量无意义的、非结构化的元素，导致后续AI生成的需求文档质量极低或完全不可用。该系统对输入源的格式有强依赖性，并不具备通用HTML页面的解析能力。

问题【4】: 系统完整工作流程详解

以下为系统从接收用户上传到最终保存测试用例的完整、详细步骤分解。

步骤 1.1: 用户上传与信息补充

输入: 用户在前端页面 (/functional-test-cases/generator) 上传一个或多个文件（必须包含至少一个.html文件），这些文件是Axure导出的产物。同时，用户输入系统名称、模块名称，并可选填写补充业务规则。

数据示例:

文件: 订单管理.html, data.js

系统名称: 电商后台

模块名称: 订单管理

补充规则: 订单金额超过10000元需要财务审批

步骤 1.2: 后端文件解析 (axureParseService.ts)

过程: 后端接收到文件后，首先识别出主HTML文件。随后，使用DOM解析库（cheerio）遍历HTML的树状结构。

提取逻辑:

元素识别: 查找带有Axure特定class（如 ax_default label, ax_default text_field）或tag（input, select）的元素。

文本提取: 提取元素的可见文本，如按钮名“查询”、输入框标签“订单号”。

类型推断: 根据元素的class、tag和文本内容，将其初步分类为 button, input, select, div 等。

业务规则提取: 专门查找长文本块（div），特别是包含“规则”、“说明”、“备注”或特定关键词（审核、校验、拦截）的文本，作为业务规则的原始素材。

输出: 一个结构化的JSON对象，包含了页面上所有被识别的元素及其属性。

数据示例:

HTML片段: <div id="u123" class="ax_default label"><div class...><p><span>查询</span></p>...</div>

解析结果 (JSON片段): { "id": "u123", "type": "button", "text": "查询" }

步骤 1.3: AI生成需求文档初稿 (functionalTestCaseAIService.ts)

过程: 将步骤1.2中生成的JSON对象、用户输入的项目信息、以及HTML文件中的长文本规则，全部打包成一个巨大的提示词（Prompt）发送给大语言模型（LLM）。

AI任务: 提示词指示AI扮演一名“需求分析专家”，其任务是：

理解页面类型: 根据元素组合（如：有“查询”按钮和表格 -> 列表页；有“保存”按钮和输入框 -> 表单页）判断页面类型。

功能模块化: 将离散的元素（输入框、按钮）组合成有业务意义的功能模块（如“查询条件区”、“操作按钮区”）。

结构化输出: 按照预设的Markdown模板，生成包含页面布局、查询条件、列表字段、操作按钮、业务规则等章节的需求文档（PRD）。

输出: 一份Markdown格式的文本。

数据示例:

输入 (JSON片段): [{ "type": "input", "name": "订单号" }, { "type": "button", "text": "查询" }]

输出 (Markdown片段):

code
Markdown
download
content_copy
expand_less
#### 1.1.2 查询条件
| 字段名 | 控件类型 | 必填 |
|---|---|---|
| 订单号 | 文本框 | 否 |

步骤 1.4: 用户审核需求文档 (第一个关键节点)

过程: 后端将生成的Markdown文档返回给前端，前端使用编辑器（MarkdownEditor.tsx）将其展示给用户。

用户操作: 用户可以阅读、编辑、补充或修正这份AI生成的需求文档。例如，补充AI未能推断出的隐藏业务规则，或修正AI对字段的错误理解。

输出: 一份经过用户确认和优化的最终版需求文档。这份文档是后续所有生成步骤的唯一依据。

步骤 2.1: 阶段一 - AI拆分测试模块

输入: 用户审核过的最终版需求文档。

过程: 用户点击“生成测试用例”后，前端将需求文档全文提交给后端。后端再次调用LLM。

AI任务: 提示词指示AI扮演一名“测试架构师”，将需求文档按功能或章节拆分为高级别的测试模块。

输出: 一个测试模块列表。

数据示例:

输入: 包含“查询条件”、“列表展示”、“操作按钮”等章节的需求文档。

输出 (JSON): [{ "id": "module-1", "name": "查询条件测试" }, { "id": "module-2", "name": "列表数据与操作测试" }]

步骤 2.2: 阶段二 - AI生成测试目的

输入: 用户从模块列表中选择一个或多个模块。

过程: 对于用户选择的每个模块，后端再次调用LLM，并将该模块关联的需求文档内容一并传入。

AI任务: 提示词指示AI扮演一名“高级测试工程师”，为指定的测试模块设计更高层级的测试目的（Test Purpose）。

输出: 每个模块下生成2-8个测试目的。

数据示例:

输入: “查询条件测试”模块。

输出 (JSON): [{ "id": "purpose-1", "name": "单条件查询验证" }, { "id": "purpose-2", "name": "多条件组合查询验证" }, { "id": "purpose-3", "name": "边界值与异常输入测试" }]

步骤 2.3: 阶段三 - AI生成测试点 (RAG增强)

这是整个系统技术含金量最高的一步。

输入: 用户从测试目的列表中选择一个或多个目的。

过程: 对于用户选择的每个测试目的，后端执行以下RAG流程：

构建查询: 将测试目的的名称和描述（如“边界值与异常输入测试”）以及相关的需求文档片段组合成一个语义查询。

向量化: 调用Embedding模型（如阿里云通义）将该查询文本转换为一个1024维的向量。

语义检索 (testCaseKnowledgeBase.ts): 使用此向量在Qdrant向量数据库的对应系统集合（test_knowledge_{系统名称}）中进行相似度搜索。

知识获取: Qdrant返回最相似的知识条目，例如，对于“边界值测试”，可能会检索到：

历史踩坑点: “特殊字符 " 和 ' 未转义导致查询失败”

风险场景: “SQL注入风险：输入 ' OR '1'='1 验证”

业务规则: “订单号长度必须为18位”

上下文增强: 将这些检索到的知识条目格式化后，注入到一个新的、最终的LLM提示词中。

最终生成调用: 将测试目的、相关需求文档内容、以及增强的知识上下文，一同发送给LLM。

AI任务: 提示词指示AI扮演一名“资深测试专家”，基于所有上下文，生成具体的、包含操作步骤、预期结果和风险等级的测试点（Test Point）。

输出: 一个包含多个测试点的完整测试用例JSON对象。

数据示例:

输入: “边界值与异常输入测试” + RAG检索到的知识。

输出 (JSON片段):

code
JSON
download
content_copy
expand_less
{
  "testPointName": "SQL注入风险测试",
  "steps": "1. 在订单号输入框输入 ' OR '1'='1\n2. 点击查询按钮",
  "expectedResult": "系统应提示查询无结果或参数无效，不应返回所有数据或报错",
  "riskLevel": "high"
}

步骤 3.1: 草稿箱展示与审核 (最后一个关键节点)

过程: 后端将生成的测试用例（包含多个测试点）返回给前端。前端以卡片形式在“草稿箱”中展示每个测试用例。

用户操作:

预览: 用户可以点击查看每个测试用例的详细测试点。

编辑: 用户可以在弹窗中直接修改测试点的步骤、预期结果等。

选择: 用户通过复选框选择最终需要保存的测试用例。

步骤 3.2: 保存到数据库

过程: 用户点击“保存到用例库”后，前端将所有选中的测试用例JSON对象批量提交给后端。

后端操作:

启动一个数据库事务。

为每个提交的测试用例，在 functional_test_cases 表中创建一条主记录。

对于该用例下的每个测试点，在 functional_test_points 表中创建一条记录，并关联到主记录的ID。

事务提交。

输出: 数据被持久化到MySQL数据库中，流程结束。用户被重定向到功能测试用例列表页面，可以看到刚刚保存的所有用例和测试点。


**Deconstructing the Query**

I'm currently focused on deconstructing the user's query. I've begun to identify the key components within their question and am working to understand the underlying intent behind their request. My primary aim is to translate this into a clear, structured framework that allows for a comprehensive, precise response, which I will then build upon.


**Analyzing User Needs**

I've successfully dissected the user's query into its fundamental parts: system identity, UI/UX flow, HCI, and user management.  My focus is now on adopting the "highly critical, skeptical expert" persona to frame a clinical, blunt analysis, mirroring a post-mortem report. The response will be factual, concise, and devoid of unnecessary language to ensure precise information delivery in direct Chinese.


**Framing the Response**

I'm now fully immersed in framing the response based on the 'expert' persona. I'm focusing on providing direct answers, each prefaced by a succinct conclusion. The key is factual accuracy and avoiding any conversational language. My plan is to start with a summary, then proceed with the sections as outlined. My goal is to deliver a precise, structured answer, formatted as a formal inquiry report.


**Defining System Functionality**

I'm solidifying the response structure. I'll include a concise intro followed by sections addressing system identity, UI/UX flow, HCI, and user management. For system identity, I've defined the tool as a workbench dedicated to the test case design and generation phase, emphasizing its core functionalities: prototype parsing, requirement generation, case generation, and draft review, and contrasting it with a full test management platform. The goal is a precise, structured response formatted as a formal inquiry report.


**Formulating Precise Answers**

I'm now fully immersed in developing the detailed responses, paying close attention to the requested "expert" persona. I've broken down each query into its core aspects to provide direct, concise answers. Each section starts with a clear confirmation and elaborates on the functionality within the system. I'll define "workbench" specifically and contrast it to test management platforms. For the "draft box", I'm formulating a precise description that clarifies its temporary state.


**Formulating Detailed Responses**

I'm developing detailed responses, incorporating the "highly critical, skeptical expert" persona. Each response starts with a clear, direct answer and then elaborates on the system's functionality. For "workbench," I'll define it precisely and contrast it with full test management platforms. The "draft box" will be defined as a temporary front-end container. I will now create the responses, each section prefaced by the requested, formal header.

I've crafted a comprehensive answer to the HCI question that details user journey interaction. It features a numbered list outlining the user's interaction from the start of uploading files to the review and editing of the generated results. My aim is to deliver this response, formatted for direct, clear consumption.


**Defining User Interactions**

I'm now focusing on a detailed HCI analysis, framed within the "highly critical, skeptical expert" persona. The response begins with a clear introduction and proceeds in numbered steps that describe the user journey and interaction with the system's inputs and outputs. It includes UI component names.

I've outlined each step, from file upload to saving, and detail the system actions and human interactions within each step. The goal is a clinical, precise report.


**Analyzing System Interactions**

I'm now outlining a detailed HCI analysis in the requested persona. The response begins with a numbered description of the user's journey from file uploads to case editing and saving within specific UI components, like MultiFileUpload.tsx, MarkdownEditor.tsx, and TestCaseDetailModal.tsx. The system actions and user interactions will be clinical and factual.


**Prioritizing User Roles**

I've formulated detailed responses, incorporating the expert persona. Each response begins with a clear answer followed by an elaboration on system functionality. I've defined "workbench" precisely and contrasted it to test management platforms. The "draft box" will be clearly defined as a temporary front-end container. I'm now composing each section with the requested, formal header.

I've written a complete response to the HCI question that outlines user interaction. It uses a numbered list to clarify the user's interaction from the start, like uploading files, to the review and editing of results generated. I am now delivering this response in a direct, clear format.


**Streamlining Response Generation**

I'm now fully immersed in developing the detailed responses, paying close attention to the requested "expert" persona. I've broken down each query into its core aspects to provide direct, concise answers. Each section starts with a clear confirmation and elaborates on the functionality within the system. I'll define "workbench" specifically and contrast it to test management platforms.


**Analyzing System Interactions**

I've outlined a detailed HCI analysis in the requested persona. The response begins with a numbered description of the user's journey from file uploads to case editing and saving within specific UI components, like MultiFileUpload.tsx, MarkdownEditor.tsx, and TestCaseDetailModal.tsx. The system actions and user interactions will be clinical and factual.


