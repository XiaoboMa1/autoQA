## 介绍

问题【1】：关于原型图用途
在传统工作流中，Axure原型图的核心功能是视觉与交互设计展示，而非作为测试用例设计的直接输入。本系统的核心假设是：一个足够详尽、遵循特定规范的Axure原型，其静态HTML产物包含了可被机器解析的、结构化的需求信息。系统并非将原型图作为交互模型来“运行”，而是将其作为一份机器可读的需求文档来“解析”。

工作机制如下：

输入: **系统接收的是Axure导出的HTML、CSS和JS文件集合，而非.rp源文件或图片。**

解析: 后端服务（axureParseService.ts）使用DOM解析库（如cheerio）遍历HTML结构。它不关心页面的视觉表现，只提取结构化信息：元素类型（通过class和tag推断）、标签文本、元素ID、data-label等Axure特有的属性。

转换: 解析后的DOM结构被转换成一份结构化的JSON对象。这份JSON对象是对页面元素的“盘点清单”，例如：{ "type": "input", "name": "用户名" }, { "type": "button", "text": "登录" }。这份JSON清单随后被提交给大语言模型（LLM），LLM的任务是基于这份清单“逆向工程”出一份产品需求文档（PRD）。系统并未改变原型图用于“设计展示”的本质。它只是利用了Axure导出产物的结构化特性，将其作为一种替代性的、不规范但可解析的需求规约来源。其有效性完全依赖于原型制作的规范性和详尽程度。

问题【2】：关于数据库结构的强制性

报告中涉及的数据库表结构是强制性的，缺少这些表，相关功能将完全无法运行。系统的数据持久化、版本控制和AI生成流程都依赖于此结构。

核心新增表结构解析：

ai_generation_sessions:

用途: 记录每一次从“上传Axure”到“生成用例”的完整会话。

关键字段: id (会话ID), user_id, axure_filename, requirement_doc (AI生成的需求文档), pre_analysis_result (智能补全分析结果), enhanced_data (用户确认后的数据)。

必要性: 确保所有AI生成过程可追溯、可审计。没有此表，无法在多步骤流程中传递上下文。

functional_test_cases:

用途: 存储高级别的测试用例，对应“测试目的”。它是一个容器。

关键字段: id, name, system, module, ai_session_id (关联生成会话)。

必要性: 组织和管理测试点，形成测试用例的层级结构。

functional_test_points:

用途: 存储具体的、可执行的测试点，是测试用例的最小单元。

关键字段: id, test_case_id (关联functional_test_cases), test_point_name, steps, expected_result, risk_level。

必要性: 这是最终产物的核心存储单元。一个functional_test_case包含多个functional_test_points。

关于向量数据库 (Qdrant):

用途: 此数据库用于实现RAG（检索增强生成）功能，是系统的增强层而非核心层。

工作机制: 它将业务规则、历史缺陷（pitfall）、风险场景等知识条目，通过Embedding模型（如阿里云通义）转换为向量并存储。在生成测试点时，系统会根据当前上下文在向量数据库中进行语义搜索，找到最相关的知识，并将其注入到LLM的提示词中，以生成更专业、更具针对性的测试点。

必要性: 如果不部署Qdrant或相关配置，系统仍然可以生成测试用例，但质量会显著下降，无法利用历史经验，也无法自动生成边界、异常和风险场景的测试。它将退化为一个纯粹基于原型内容推断的生成器。

问题【3】：“成熟原型”与“任意HTML”的界定

你的理解是正确的，系统接收的是HTML输入。然而，“理论上可以把任何HTML丢进去”这一推论是不成立的。

原因如下：

解析器高度耦合: axureParseService.ts中的解析逻辑是为Axure导出的特定HTML结构量身定制的。它依赖于Axure生成的特定class名称、ID格式和data-*属性。

示例: 系统通过查找div.ax_default.label来识别按钮和标签，并通过文本长度和关键词来区分两者。一个标准的网页（如<div><button>提交</button></div>）不包含这种结构，会导致按钮识别失败。

结构化假设: 系统假设HTML内容在结构上反映了功能的组织。例如，它会假定“查询”按钮附近的输入框是查询条件，而“保存”按钮附近的输入框是表单字段。普通网页的布局可能不遵循这种简单的逻辑。

“成熟原型”的定义: 在此项目的语境下，“成熟”指的不是原型在设计评审流程中的状态，而是其内容的详尽和规范程度。一个“成熟”的Axure原型意味着：

所有可交互元素（按钮、输入框）都已绘制。

所有元素的标签文本都已正确填写。

页面之间的跳转关系已通过交互链接建立。

复杂的业务规则已通过注释或文本块的形式在原型中说明。

结论: 将任意网站的HTML输入此系统，解析器会因找不到预期的结构模式而失败，或提取出大量无意义的、非结构化的元素，导致后续AI生成的需求文档质量极低或完全不可用。该系统对输入源的格式有强依赖性，并不具备通用HTML页面的解析能力。

问题【4】: 系统完整工作流程详解

以下为系统从接收用户上传到最终保存测试用例的完整、详细步骤分解。

步骤 1.1: 用户上传与信息补充

输入: 用户在前端页面 (/functional-test-cases/generator) 上传一个或多个文件（必须包含至少一个.html文件），这些文件是Axure导出的产物。同时，用户输入系统名称、模块名称，并可选填写补充业务规则。

数据示例:

文件: 订单管理.html, data.js

系统名称: 电商后台

模块名称: 订单管理

补充规则: 订单金额超过10000元需要财务审批

步骤 1.2: 后端文件解析 (axureParseService.ts)

过程: 后端接收到文件后，首先识别出主HTML文件。随后，使用DOM解析库（cheerio）遍历HTML的树状结构。

提取逻辑:

元素识别: 查找带有Axure特定class（如 ax_default label, ax_default text_field）或tag（input, select）的元素。

文本提取: 提取元素的可见文本，如按钮名“查询”、输入框标签“订单号”。

类型推断: 根据元素的class、tag和文本内容，将其初步分类为 button, input, select, div 等。

业务规则提取: 专门查找长文本块（div），特别是包含“规则”、“说明”、“备注”或特定关键词（审核、校验、拦截）的文本，作为业务规则的原始素材。

输出: 一个结构化的JSON对象，包含了页面上所有被识别的元素及其属性。

数据示例:

HTML片段: <div id="u123" class="ax_default label"><div class...><p><span>查询</span></p>...</div>

解析结果 (JSON片段): { "id": "u123", "type": "button", "text": "查询" }

步骤 1.3: AI生成需求文档初稿 (functionalTestCaseAIService.ts)

过程: 将步骤1.2中生成的JSON对象、用户输入的项目信息、以及HTML文件中的长文本规则，全部打包成一个巨大的提示词（Prompt）发送给大语言模型（LLM）。

AI任务: 提示词指示AI扮演一名“需求分析专家”，其任务是：

理解页面类型: 根据元素组合（如：有“查询”按钮和表格 -> 列表页；有“保存”按钮和输入框 -> 表单页）判断页面类型。

功能模块化: 将离散的元素（输入框、按钮）组合成有业务意义的功能模块（如“查询条件区”、“操作按钮区”）。

结构化输出: 按照预设的Markdown模板，生成包含页面布局、查询条件、列表字段、操作按钮、业务规则等章节的需求文档（PRD）。

输出: 一份Markdown格式的文本。

数据示例:

输入 (JSON片段): [{ "type": "input", "name": "订单号" }, { "type": "button", "text": "查询" }]

输出 (Markdown片段):

code
Markdown
download
content_copy
expand_less
#### 1.1.2 查询条件
| 字段名 | 控件类型 | 必填 |
|---|---|---|
| 订单号 | 文本框 | 否 |

步骤 1.4: 用户审核需求文档 (第一个关键节点)

过程: 后端将生成的Markdown文档返回给前端，前端使用编辑器（MarkdownEditor.tsx）将其展示给用户。

用户操作: 用户可以阅读、编辑、补充或修正这份AI生成的需求文档。例如，补充AI未能推断出的隐藏业务规则，或修正AI对字段的错误理解。

输出: 一份经过用户确认和优化的最终版需求文档。这份文档是后续所有生成步骤的唯一依据。

步骤 2.1: 阶段一 - AI拆分测试模块

输入: 用户审核过的最终版需求文档。

过程: 用户点击“生成测试用例”后，前端将需求文档全文提交给后端。后端再次调用LLM。

AI任务: 提示词指示AI扮演一名“测试架构师”，将需求文档按功能或章节拆分为高级别的测试模块。

输出: 一个测试模块列表。

数据示例:

输入: 包含“查询条件”、“列表展示”、“操作按钮”等章节的需求文档。

输出 (JSON): [{ "id": "module-1", "name": "查询条件测试" }, { "id": "module-2", "name": "列表数据与操作测试" }]

步骤 2.2: 阶段二 - AI生成测试目的

输入: 用户从模块列表中选择一个或多个模块。

过程: 对于用户选择的每个模块，后端再次调用LLM，并将该模块关联的需求文档内容一并传入。

AI任务: 提示词指示AI扮演一名“高级测试工程师”，为指定的测试模块设计更高层级的测试目的（Test Purpose）。

输出: 每个模块下生成2-8个测试目的。

数据示例:

输入: “查询条件测试”模块。

输出 (JSON): [{ "id": "purpose-1", "name": "单条件查询验证" }, { "id": "purpose-2", "name": "多条件组合查询验证" }, { "id": "purpose-3", "name": "边界值与异常输入测试" }]

步骤 2.3: 阶段三 - AI生成测试点 (RAG增强)

这是整个系统技术含金量最高的一步。

输入: 用户从测试目的列表中选择一个或多个目的。

过程: 对于用户选择的每个测试目的，后端执行以下RAG流程：

构建查询: 将测试目的的名称和描述（如“边界值与异常输入测试”）以及相关的需求文档片段组合成一个语义查询。

向量化: 调用Embedding模型（如阿里云通义）将该查询文本转换为一个1024维的向量。

语义检索 (testCaseKnowledgeBase.ts): 使用此向量在Qdrant向量数据库的对应系统集合（test_knowledge_{系统名称}）中进行相似度搜索。

知识获取: Qdrant返回最相似的知识条目，例如，对于“边界值测试”，可能会检索到：

历史踩坑点: “特殊字符 " 和 ' 未转义导致查询失败”

风险场景: “SQL注入风险：输入 ' OR '1'='1 验证”

业务规则: “订单号长度必须为18位”

上下文增强: 将这些检索到的知识条目格式化后，注入到一个新的、最终的LLM提示词中。

最终生成调用: 将测试目的、相关需求文档内容、以及增强的知识上下文，一同发送给LLM。

AI任务: 提示词指示AI扮演一名“资深测试专家”，基于所有上下文，生成具体的、包含操作步骤、预期结果和风险等级的测试点（Test Point）。

输出: 一个包含多个测试点的完整测试用例JSON对象。

数据示例:

输入: “边界值与异常输入测试” + RAG检索到的知识。

输出 (JSON片段):

code
JSON
download
content_copy
expand_less
{
  "testPointName": "SQL注入风险测试",
  "steps": "1. 在订单号输入框输入 ' OR '1'='1\n2. 点击查询按钮",
  "expectedResult": "系统应提示查询无结果或参数无效，不应返回所有数据或报错",
  "riskLevel": "high"
}

步骤 3.1: 草稿箱展示与审核 (最后一个关键节点)

过程: 后端将生成的测试用例（包含多个测试点）返回给前端。前端以卡片形式在“草稿箱”中展示每个测试用例。

用户操作:

预览: 用户可以点击查看每个测试用例的详细测试点。

编辑: 用户可以在弹窗中直接修改测试点的步骤、预期结果等。

选择: 用户通过复选框选择最终需要保存的测试用例。

步骤 3.2: 保存到数据库

过程: 用户点击“保存到用例库”后，前端将所有选中的测试用例JSON对象批量提交给后端。

后端操作:

启动一个数据库事务。

为每个提交的测试用例，在 functional_test_cases 表中创建一条主记录。

对于该用例下的每个测试点，在 functional_test_points 表中创建一条记录，并关联到主记录的ID。

事务提交。

输出: 数据被持久化到MySQL数据库中，流程结束。用户被重定向到功能测试用例列表页面，可以看到刚刚保存的所有用例和测试点。


**关于系统定位：**
是一个专用于测试用例设计与生成阶段的辅助工具集，即一个“工作台”，它并非一个全面的、端到端的测试管理平台（如Jira+Xray, TestRail），因为它**不直接处理测试执行跟踪、缺陷管理或测试报告的完整生命周期**。

其核心功能限定在以下范围内：

原型解析: 将Axure原型HTML作为结构化数据源进行解析。

需求生成: 基于解析结果，利用LLM生成一份可读的需求文档初稿。

用例生成: 基于需求文档，通过三阶段渐进式流程生成测试模块、测试目的和具体的测试点。

草稿审核: 在“AI生成器”页面 (/functional-test-cases/generator) 的第三个步骤（生成用例）中，用于临时展示和管理AI生成结果的前端状态容器。供用户审核、编辑和选择生成的用例。

它仅存在于AI生成器页面的第三步界面中。临时性: 其内容存储在前端应用的内存状态（React State）中，并可能缓存到浏览器的LocalStorage以防止意外关闭。这些数据在用户点击“保存到用例库”或离开页面后即被清空或失效。交互性: 用户在此区域内对AI生成的测试用例卡片进行交互，包括：通过复选框选择或取消选择。点击卡片打开详情弹窗（TestCaseDetailModal.tsx）进行编辑。触发“重新生成”、“优化”等批量操作。它的唯一目的是作为将AI生成结果持久化到数据库（functional_test_cases 和 functional_test_points 表）之前的缓冲区和审核区。

**关于输入输出与人类的交互流程**
完全通过其Web前端界面与用户进行交互。以下是完整的交互路径：

输入 (Input):

动作: 用户通过前端界面 (src/components/ai-generator/MultiFileUpload.tsx 组件) 上传文件。

人类交互: 用户将本地的Axure导出文件夹（包含.html和.js文件）拖拽到上传区域，或通过文件选择器选取。同时，在表单中输入“系统名称”、“模块名称”等文本信息。

中间输出 / 交互 (Intermediate Output / Interaction):

动作: 系统完成解析后，输出AI生成的需求文档初稿。

人类交互: 用户在网页上的一个富文本编辑器（src/components/ai-generator/MarkdownEditor.tsx）中阅读和编辑这份Markdown格式的文档。这是第一个关键的人机交互审核点。

最终输出 / 交互 (Final Output / Interaction):

动作: 在用户确认需求文档后，系统在“草稿箱”区域以卡片形式（src/components/ai-generator/DraftCaseCard.tsx）分批输出生成的测试用例。

人类交互: 用户浏览这些卡片，通过点击复选框进行选择。如果需要修改，可以点击卡片打开一个模态框（src/components/ai-generator/TestCaseDetailModal.tsx）来编辑测试点的具体步骤和预期结果。这是第二个关键的人机交互审核点。

持久化 (Persistence):

动作: 用户完成选择和编辑后，点击“保存到用例库”按钮。

人类交互: 单次点击操作，触发数据持久化。

最终结果展示 (Final Result Display):

动作: 系统将用户保存的测试用例持久化到数据库后，通常会跳转到功能测试用例列表页面 (/functional-test-cases)。

人类交互: 用户在该页面以表格或列表的形式，查看已成为正式记录的、可供后续使用的测试用例。

**用户系统、角色与权限**
系统具备完整的用户登录、角色与权限管理机制。

4.1 用户登录系统
存在性: 是。文件结构中明确包含用户认证相关模块：

server/routes/auth.ts (登录API路由)

server/services/authService.ts (登录逻辑服务)

src/pages/Login.tsx (前端登录页面)

src/contexts/AuthContext.tsx (前端认证状态管理)

机制: 系统基于JWT (JSON Web Token) 实现会话管理。用户登录成功后，前端会存储一个token用于后续所有API请求的身份验证。

4.2 目标用户角色

主要用户: 根据需求文档 docs/tech-docs/Axure自动生成测试用例-需求文档-V2.0-最终版.md 中的“1.4 用户角色”章节定义，系统的主要用户是测试工程师、QA工程师。

次要用户: 产品经理和开发工程师被定义为次要用户，他们可以使用系统来了解测试覆盖范围和测试场景。

4.3 角色与权限

存在性: 是。系统定义了不同级别的权限，主要体现在数据隔离上。

证据:

数据库层面: prisma/schema.prisma 文件中的 users 表定义了 is_super_admin (布尔型) 字段，表明存在超级管理员角色。同时，users, test_cases, functional_test_cases 等多个表都包含 department (部门) 字段，这是实现数据隔离的基础。

代码层面: 后端路由和服务中存在基于用户角色和部门进行数据过滤的逻辑。例如，在获取测试用例列表时，会检查用户是否为 isSuperAdmin。如果不是，查询将额外附加 WHERE department = '用户的部门' 的条件。

定义的角色:

超级管理员 (Super Admin): 拥有最高权限，可以查看和管理所有部门的测试数据。

普通用户 (Regular User): 权限受其所属 department 字段限制，只能查看和管理自己所在部门的测试用例和测试套件。


**系统中AI的角色、数量及数据处理流程**

系统中存在两个不同阶段、承担不同任务的LLM调用，而非单一AI。
AI #1: 需求文档生成器 (PRD Generator)
触发阶段: 用户上传Axure原型文件后。
输入: axureParseService.ts 服务输出的结构化JSON对象。此解析过程完全由确定性代码（cheerio库）完成，不涉及任何AI。它遍历HTML DOM，提取元素、文本和Axure特有属性，生成一份对页面元素的“盘点清单”。
任务: 此AI的核心任务是将这份离散的、缺乏业务上下文的元素清单，“逆向工程”成一份人类可读的、结构化的Markdown格式产品需求文档（PRD）。它负责推断页面类型（列表页/表单页）、将元素组织成功能模块（查询区/操作区），并初步整理业务规则。
输出: Markdown文本格式的PRD初稿。

AI #2: 测试点生成器 (Test Point Generator)

触发阶段: 在我（作为用户）审核并确认了AI #1生成的PRD之后。
输入: 经我人工审核和修正后的最终版PRD中的特定章节内容，以及一个明确的“测试目的”（例如，“验证单条件查询功能”）。
任务: 此AI的核心任务是扮演一名“资深测试专家”，基于给定的需求文档片段和测试目的，设计出具体的、可执行的测试点。每个测试点都包含明确的steps（操作步骤）和expectedResult（预期结果）。这才是第三方方案中“生成步骤”所指的环节。
增强机制 (RAG): 在调用此AI之前，系统会先执行一次RAG（检索增强生成）流程。它将“测试目的”和相关需求文本作为查询，在Qdrant向量数据库中搜索相关的历史经验（如业务规则、历史缺陷），并将检索到的知识注入到此AI的提示词中，以提高生成测试点的专业性和覆盖度。
输出: 结构化的JSON对象，其中包含一个测试点数组 (testPoints)。

结论: Axure原型的解析是100%的确定性代码，其准确性应通过传统的单元测试和集成测试来保证，而非AI评估。第三方方案真正想要验证的，应该是**AI #2（测试点生成器）**的输出质量。

==================================
## 哪些是亮点，哪些是行业普遍操作？

（哪些是标准的合格的开发者应该主动做的，而不是真正的技术挑战或设计亮点。

1. 基于MCP协议的浏览器自动化抽象层
1.1 设计动因与解决的问题

****在构建基于LLM的自动化测试执行器时，核心挑战在于如何规范化模型的输出。直接让LLM生成Playwright代码存在极高的安全风险（代码注入）和不稳定性（幻觉）。
决策逻辑： 我们采用了Model Context Protocol (MCP) 作为中间抽象层，而非直接生成代码。
**行业普遍实践部分：**
不直接生成和执行LLM产出的代码：这是最基本的安全常识。任何允许LLM直接生成可执行代码而没有严格沙箱和审查机制的系统，都存在严重的安全漏洞。选择一个抽象层（无论是MCP、API调用还是其他形式的工具调用）是避免代码注入风险的标准做法，是必须做的，而不是一个亮点。
使用Schema验证：对任何外部输入（包括LLM的输出）使用Zod、Pydantic或类似的工具进行严格的Schema验证，是构建健壮系统的标准操作。这确保了数据结构的确定性，防止了格式错误。
将浏览器操作原子化为一系列“工具”，并设计这些工具的接口，这个工具集的设计本身是有价值的

上下文管理： 解决了Playwright上下文与LLM Token窗口之间的状态同步问题，通过 list_tools 动态暴露当前页面可用的操作。

1.2 关键代码引用： PlaywrightMcpClient 如何实现MCP协议的工具调用请求，将抽象的工具名映射为通过传输层发送的标准化JSON-RPC消息。

code
TypeScript
download
content_copy
expand_less
// 文件: backend/src/services/mcp/PlaywrightMcpClient.ts

async callTool(toolName: string, toolArgs: any): Promise<any> {
    // 构建符合MCP规范的工具调用请求
    // 这一步将业务逻辑与底层传输解耦，确保发送的数据符合 CallToolRequestSchema 契约
    const result = await this.client.request(
      {
        method: "tools/call", // 标准MCP方法名
        params: {
          name: toolName,     // 映射到 Playwright 的具体操作（如 'click'）
          arguments: toolArgs // 经过 Zod 验证的参数对象
        },
      },
      CallToolRequestSchema // Zod Schema 用于运行时响应验证
    );

    return result;
}
**2. 递归式DOM快照与选择器解析策略**
2.1 设计动因与解决的问题

LLM无法直接“看”到渲染后的网页。传统的做法是将整个HTML传给LLM，但这会导致Token溢出且包含大量无意义的样式噪音。
决策逻辑： 设计了 AITestParser 服务，实施了“快照压缩-意图匹配”策略。

**行业普遍实践部分：**
对输入数据进行预处理和清洗：将原始HTML直接喂给LLM是极其低效且昂贵的做法。提取关键信息、移除噪音（如样式、脚本）是任何RAG或信息提取应用中的标准步骤。这属于数据工程中的“特征提取”。
提取可交互元素：在Web自动化场景下，首先关注<a>, <button>, <input>等元素，这是最直接的思路，是标准做法。

**值得一提的部分：**
“意图映射”：声称能将模糊指令（“登录系统”）转换为具体的Playaright选择器操作，这个过程是真正有挑战性的。这背后通常需要一个语义搜索、分类或更复杂的Prompt Chain。如何实现从自然语言到具体UI元素引用的精确映射，而不是“我们没有把整个HTML都发给LLM”这件事本身。

2.2 关键代码引用

代码展示了如何整合页面上下文（URL、标题）与测试步骤描述，为LLM构建最小必要上下文。

code
TypeScript
download
content_copy
expand_less
// 文件: backend/src/services/ai/AiTestParser.ts

export class AITestParser {
  // ... (省略部分代码)
  
  async parseTestStep(
    stepDescription: string, // 用户输入的自然语言步骤
    pageSnapshot: string,    // 经过清洗的轻量级DOM快照
    currentUrl: string,      
    pageTitle: string
  ): Promise<{
    toolCalls: Array<{ name: string; arguments: any }>; // 解析出的标准化工具调用数组
    thoughtProcess: string; // 模型的思维链，用于调试
  }> {
    // ... (省略Prompt构建逻辑)
    
    // 调用LLM，要求其基于当前压缩后的快照解析用户意图
    const response = await this.llmProvider.generateCompletion(prompt);
    
    // ... (省略解析逻辑)
  }
}


**4. 基于MJPEG流的无头浏览器实时透传**
4.1 设计动因与解决的问题
Playwright在服务端以Headless模式运行，用户无法直观感知执行进度。WebRTC方案过于重型且需要复杂的ICE穿透配置。
决策逻辑： 采用MJPEG (Motion JPEG) 流技术。
**行业普遍实践部分：**
选择MJPEG：MJPEG over HTTP multipart response是一种非常古老、简单且广泛使用的视频流技术，常用于网络摄像头等场景。选择它而不是更复杂的WebRTC或WebSocket二进制流，是一个基于“简单、够用”原则的务实工程决策，但技术本身非常标准。
**值得一提的部分：**
与CDP事件流集成：亮点在于如何获取视频帧。直接挂钩到Playwright底层的CDP (Chrome DevTools Protocol) Page.screencastFrame事件流来获取屏幕帧，这表明开发者对Playwright工具有更深层次的理解

4.2 关键代码引用：建立HTTP响应流，并处理浏览器的 screencastFrame 事件，实现帧的实时转发。

code
TypeScript
download
content_copy
expand_less
// 文件: backend/src/services/stream/StreamService.ts

async startStream(res: Response, runId: string) {
    // 设置 MJPEG 标准响应头，告知浏览器这是一个持续更新的流
    res.writeHead(200, {
      "Content-Type": "multipart/x-mixed-replace; boundary=frame",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    });

    // 获取对应的 CDP 会话
    const session = this.sessions.get(runId);
    
    // 监听 CDP 的截屏帧事件
    session.on("Page.screencastFrame", async ({ data, sessionId }) => {
      // 确认帧已接收，维持 CDP 会话活性
      await session.send("Page.screencastFrameAck", { sessionId });
      
      // 封装 JPEG 帧数据并写入 HTTP 响应流
      const frameData = Buffer.from(data, "base64");
      res.write(`--frame\r\nContent-Type: image/jpeg\r\nContent-Length: ${frameData.length}\r\n\r\n`);
      res.write(frameData);
      res.write("\r\n");
    });
}

**5. 具备并发熔断机制的任务队列管理**
5.1 设计动因与解决的问题

浏览器实例是内存密集型进程。若无限制地开启测试任务，会导致服务器OOM（内存溢出）宕机。简单的 Promise.all 无法控制最大并发数。
决策逻辑： 引入 p-queue 库构建优先级队列，并配合 Map 管理任务状态。
**行业普遍实践部分：**
控制资源密集型任务的并发：启动浏览器实例是典型的资源密集操作。如果不加控制地并发执行，服务器内存溢出是必然结果。因此，使用任务队列（如Bull, Celery, RabbitMQ）来管理任务并限制并发，是任何生产系统的基本要求。
使用p-queue等库：在Node.js环境中，使用p-queue或类似库来管理Promise的并发是标准实践，没人会手写一个任务调度器。
将并发数硬编码为1，这是一个极度保守的配置，确保了绝对的稳定性但牺牲了吞吐量。这是一个业务决策，而非技术亮点。做了最基本的服务器资源保护，是合格的后端开发者应该做的。这算不上亮点，而是避免了灾难。
状态一致性： 通过 activeRuns Map 追踪正在运行的任务，防止对已完成或已失败的任务进行重复操作。

**亮点：根据服务器负载动态调整并发数**
实现动态并发调整的难度不复杂，但在工程实践中有中等难度。主要挑战在于：
**定义“负载”：**你需要明确哪些指标（CPU、内存、事件循环延迟）能准确反映系统瓶颈。**确定阈值：**找到触发并发数增加或减少的合理阈值需要反复测试和调优。阈值过低会导致资源浪费，过高则可能导致系统不稳定。控制逻辑：实现一个稳定、无抖动的控制循环，避免因瞬时负载波动而频繁调整并发数。
具体陈述见下方 #### 根据服务器负载动态调整并发数

**6.  任务分解下的测试生成过程**
6.1 设计动因与解决的问题
直接要求LLM“生成测试用例”往往得到泛泛而谈的结果。复杂的业务逻辑需要分拆处理。
**行业普遍实践：**
任务分解（Decomposition）：将一个复杂的LLM任务（“生成测试用例”）分解为一系列更小、更简单的子任务（生成需求 -> 划分模块 -> 生成测试点），这是LLM Prompt工程中非常标准且有效的策略，通常被称为“Chain of Thought”或任务分解。
标准的Prompt工程：RequirementDoc -> TestModules -> TestPoints对业务领域的理解。

以下代码片段展示了第三阶段（测试点生成）如何依赖前序阶段的数据（模块列表）进行迭代生成。
// 文件: backend/src/services/ai/FunctionalTestCaseAIService.ts

async generateTestPoints(
    requirementDoc: string,
    modules: TestModule[] // 依赖上一阶段生成的模块列表
): Promise<TestPoint[]> {
    const allTestPoints: TestPoint[] = [];

    // 针对每个模块独立生成测试点，而非一次性生成所有
    // 这种策略降低了单次 LLM 请求的复杂度
    for (const module of modules) {
      const prompt = `
        Based on the requirements:
        ${requirementDoc}
        
        Generate test points for module: ${module.name}
        Description: ${module.description}
        ...
      `;
      // ... (调用 LLM 并累加结果)
    }
    return allTestPoints;
}

7. 基于JSON Patch (RFC 6902) 的原子化更新
7.1 设计动因与解决的问题
用户经常需要批量修改测试步骤。传统的全量覆盖更新（PUT）容易引发数据竞争，且难以追踪具体的修改项。此外，可以最小化传输： 前端只需发送操作指令（如 replace path="/steps/0/value"），而非整个对象，减少网络负载。

**行业普遍实践：**
在RESTful API中，使用PATCH方法进行资源的部分更新是一种标准。而JSON Patch (RFC 6902)是实现PATCH操作的官方标准化格式之一。选择它而不是发明自己的格式，是遵循行业标准的体现。使用成熟的库（如fast-json-patch）来处理这种标准格式也是常规操作。
代码展示了如何应用Patch操作到原始测试用例上，实现了非破坏性的更新。
// 文件: backend/src/services/ai/AiBulkUpdateService.ts

import * as jsonpatch from "fast-json-patch";

// ...

// 应用补丁：输入原始文档和 patch 操作数组，输出新文档
const newDocument = jsonpatch.applyPatch(
    originalDocument,
    patchOperations // 符合 RFC 6902 标准的操作数组
).newDocument;

// 这种方式使得更新操作是确定性的，并且可以通过 verifyPatch 预先检查合法性

**8. 测试制品持久化**
8.1 设计动因与解决的问题
测试执行不仅仅是Pass/Fail的结果，更重要的是可追溯的证据（截图、Trace文件）。这些二进制文件通常体积较大，不适合直接存入数据库。
决策逻辑： 实现 EvidenceService，分离元数据存储（DB）与大文件存储（文件系统/对象存储）。
**行业普遍实践：**
分离元数据和二进制文件：将大文件（图片、视频、zip包）存储在文件系统或对象存储（如S3）中，而在数据库中只存储其元数据（路径、ID、时间戳等），这是应用架构设计的基本原则。直接将二进制大对象（BLOB）存入关系型数据库是公认的反模式。
**值得一提的部分：**
保存Playwright Trace（.zip）文件：这个决策本身是亮点。它表明开发者不仅仅满足于“测试通过/失败”，而是深入利用了Playwright的调试能力，允许开发者在本地以时间轴方式完全重放测试执行过程，复现Flaky Tests。

代码引用：文件路径的规范化处理以及 Trace 文件的保存逻辑。

// 文件: backend/src/services/evidence/EvidenceService.ts

// 构建基于 runId 的隔离存储目录
const evidenceDir = path.join(process.cwd(), "evidence", runId);

// ...

// 保存 Trace 文件
// Playwright 上下文追踪停止时导出 trace 包
await context.tracing.stop({
    path: path.join(evidenceDir, "trace.zip"),
});

// 记录元数据到内存或数据库，指向物理文件路径
this.evidenceStore.set(runId, {
    screenshots: screenshotPaths,
    tracePath: `/evidence/${runId}/trace.zip`, // 相对路径，用于前端访问
    logs: logs,
});

**9. 多租户架构下的知识库隔离**
9.1 设计动因与解决的问题
在SaaS模式或多项目环境中，AI很容易混淆不同项目的业务规则。如果不仅行物理或逻辑隔离，AI可能会用A项目的登录账号去测试B项目。
决策逻辑： 在 TestCaseKnowledgeBase 中强制实施基于 systemName 的命名空间策略。索引级隔离： 在创建向量集合（Collection）时或查询时，始终携带系统标识。相关性提升： 缩小了向量检索的搜索空间（Search Space），使得召回的上下文不仅语义相关，而且业务域正确。

**行业普遍实践：**
使用元数据过滤实现数据隔离：在向量数据库中，通过在Payload/Metadata中存储systemName等字段，并在检索时使用filter参数，是实现多租户或多项目数据隔离的标准且唯一推荐的方法。未展示任何超出标准实践的创新。

代码显示了在添加文档到知识库时，如何构建包含系统标识的Payload，为后续的过滤查询打下基础。
// 文件: backend/src/services/knowledgeBase/TestCaseKnowledgeBase.ts

async addTestCase(
    testCase: string,
    category: string,
    systemName: string // 关键参数：系统标识
): Promise<void> {
    // ...
    await this.client.upsert(this.collectionName, {
      points: [
        {
          id: uuidv4(),
          vector: embedding,
          payload: {
            content: testCase,
            category,
            system: systemName, // 将系统名固化到向量 Payload 中
            timestamp: Date.now(),
          },
        },
      ],
    });
}
**10. 事件驱动的实时状态同步 (WebSocket)**
10.1 设计动因与解决的问题
HTTP轮询（Polling）状态（如“测试中”、“已完成”）会造成不必要的网络开销和数据库压力，且延迟较高。
决策逻辑： 集成 socket.io 实现全双工通信。 测试执行的每一个步骤（启动、步骤成功、步骤失败、结束）都作为事件实时推送到前端，用户感知无延迟。支持多用户协同，一个用户触发测试，其他查看同一页面的用户也能实时看到状态变化。

**行业普遍实践：**
当需要服务器向客户端实时推送信息时，替代方案是HTTP轮询、长轮询、服务器发送事件（SSE）和WebSocket。对于需要低延迟、双向通信的场景，WebSocket是标准的、主流的技术选型。
一个健壮的WebSocket实现（包括心跳、自动重连、按runId分发消息的“房间”或“频道”机制）是值得一提的。代码片段中只展示了一个简单的全局广播（this.io.emit），这是最基础的用法。如果实现了更复杂的频道管理，那才是亮点。

代码展示了WebSocket服务的单例封装及状态广播方法的实现。

code
TypeScript
download
content_copy
expand_less
// 文件: backend/src/services/websocket/WebSocketService.ts

broadcastStatus(runId: string, status: any) {
    // 向所有连接的客户端广播特定 runId 的状态更新事件
    // 前端监听 'executionStatus' 事件即可实现进度条或状态图标的实时刷新
    this.io.emit("executionStatus", {
      runId,
      ...status,
      timestamp: Date.now(),
    });
}

#### 

专为本地开发环境（单机）设计，不依赖 Docker/K8s，但能有效防止系统因浏览器实例过多而崩溃。

1. 系统设计架构
该设计遵循**反馈控制环路（Feedback Control Loop）**模型：

传感器（Sensors）：

宿主负载 (Host Load)：使用 os.loadavg()（Unix）或 os.cpus() 快照（Windows兼容）获取系统级 CPU 压力。

宿主内存 (Host Memory)：使用 os.freemem() 监控剩余物理内存，这是防止 OOM 的硬指标。

主线程健康度 (Event Loop Lag)：监控 Node.js 自身调度能力，防止指令发送阻塞。

控制器（Controller）：

冷却机制 (Cooldown)：执行调整后，锁定一段时间（如 5秒），等待系统负载产生变化（Hysteresis），防止震荡。

AIMD 策略：和性增加（一次加1），乘性减少（一次减半或降至最低），对危险信号快速反应。

执行器（Actuator）：

直接修改 p-queue 实例的 concurrency 属性。

2. 代码实现
文件 1: SystemMonitor.ts (传感器层)
解决了 Windows 下 loadavg 为 0 的问题，并实现了全局内存和延迟监控。

TypeScript

import os from 'os';

export class SystemMonitor {
    // 计算 Event Loop Lag
    public getEventLoopLag(): Promise<number> {
        return new Promise((resolve) => {
            const start = process.hrtime();
            setImmediate(() => {
                const diff = process.hrtime(start);
                const lag = (diff[0] * 1e9 + diff[1]) / 1e6; // 毫秒
                resolve(lag);
            });
        });
    }

    // 获取系统内存使用率 (0.0 - 1.0)
    // 越高越危险
    public getMemoryUsageRatio(): number {
        const total = os.totalmem();
        const free = os.freemem();
        return (total - free) / total;
    }

    // 获取系统 CPU 负载 (0.0 - 1.0+)
    // 归一化：LoadAvg / CPU核数
    // 兼容 Windows (Windows 下 loadavg 通常为 0，需用 cpus snapshot)
    public async getSystemLoadRatio(): Promise<number> {
        const platform = os.platform();
        
        if (platform === 'win32') {
            // Windows 简易实现：采样 200ms 计算 CPU 利用率
            const startCpus = os.cpus();
            await new Promise(r => setTimeout(r, 200));
            const endCpus = os.cpus();
            
            let idleDiff = 0;
            let totalDiff = 0;
            
            for (let i = 0; i < startCpus.length; i++) {
                const start = startCpus[i].times;
                const end = endCpus[i].times;
                
                const idle = end.idle - start.idle;
                const total = (end.user + end.nice + end.sys + end.idle + end.irq) - 
                              (start.user + start.nice + start.sys + start.idle + start.irq);
                
                idleDiff += idle;
                totalDiff += total;
            }
            return 1 - (idleDiff / totalDiff);
        } else {
            // Unix/Mac 使用 loadavg(1min) / 核数
            const cpus = os.cpus().length;
            const loadAvg1Min = os.loadavg()[0];
            return loadAvg1Min / cpus;
        }
    }
}
文件 2: AdaptiveQueue.ts (控制器与执行器)
包含冷却逻辑和阈值判断。

TypeScript

import PQueue from 'p-queue';
import { SystemMonitor } from './SystemMonitor';

interface AdaptiveOptions {
    minConcurrency: number;
    maxConcurrency: number;
    checkIntervalMs: number;
    cooldownMs: number;
    thresholds: {
        memHigh: number; // e.g., 0.85 (85% RAM used)
        cpuHigh: number; // e.g., 0.80 (80% CPU Load)
        lagHigh: number; // e.g., 50 (50ms Lag)
    }
}

export class AdaptiveQueue {
    public queue: PQueue;
    private monitor: SystemMonitor;
    private options: AdaptiveOptions;
    private lastAdjustTime: number = 0;
    private intervalId: NodeJS.Timeout | null = null;

    constructor(options: AdaptiveOptions) {
        this.options = options;
        this.queue = new PQueue({ concurrency: options.minConcurrency });
        this.monitor = new SystemMonitor();
    }

    public startMonitoring() {
        this.intervalId = setInterval(() => this.adjust(), this.options.checkIntervalMs);
        console.log('[AdaptiveQueue] Monitoring started.');
    }

    public stopMonitoring() {
        if (this.intervalId) clearInterval(this.intervalId);
    }

    private async adjust() {
        const now = Date.now();
        // 1. 冷却检查
        if (now - this.lastAdjustTime < this.options.cooldownMs) {
            return;
        }

        // 2. 获取指标
        const memRatio = this.monitor.getMemoryUsageRatio();
        const cpuRatio = await this.monitor.getSystemLoadRatio();
        const lag = await this.monitor.getEventLoopLag();

        const current = this.queue.concurrency;
        let next = current;
        let reason = '';

        // 3. 决策逻辑 (优先处理过载)
        const isOverloaded = 
            memRatio > this.options.thresholds.memHigh || 
            cpuRatio > this.options.thresholds.cpuHigh || 
            lag > this.options.thresholds.lagHigh;

        if (isOverloaded) {
            // 降级策略：直接降到最小或减半 (快降)
            next = Math.max(this.options.minConcurrency, Math.floor(current / 2));
            reason = `OVERLOAD (Mem: ${(memRatio*100).toFixed(1)}%, CPU: ${(cpuRatio*100).toFixed(1)}%, Lag: ${lag.toFixed(1)}ms)`;
        } else {
            // 升级策略：资源充裕时，线性增加 (慢升)
            // 留出 10% 的 buffer，不要太激进
            const isSafe = 
                memRatio < (this.options.thresholds.memHigh - 0.1) && 
                cpuRatio < (this.options.thresholds.cpuHigh - 0.2) && 
                lag < 10;

            if (isSafe && current < this.options.maxConcurrency) {
                next = current + 1;
                reason = `SAFE (Mem: ${(memRatio*100).toFixed(1)}%, CPU: ${(cpuRatio*100).toFixed(1)}%)`;
            }
        }

        // 4. 执行调整
        if (next !== current) {
            this.queue.concurrency = next;
            this.lastAdjustTime = now;
            console.log(`[ADJUST] Concurrency ${current} -> ${next} | ${reason}`);
        } else {
            // 可选：打印心跳日志证明监控在运行
            // console.log(`[MONITOR] Stable at ${current}. Load: ${(cpuRatio*100).toFixed(0)}%`);
        }
    }
}
3. 验证过程（本地实战）
为了验证该机制是否有效，不能仅靠“打印日志”，必须制造真实的资源竞争。

步骤 A: 编写负载模拟脚本 (verify.ts)
此脚本模拟两类负载：

内存泄漏：模拟浏览器打开页面消耗内存。

CPU 阻塞：模拟页面渲染占用 CPU。

TypeScript

// verify.ts
import { AdaptiveQueue } from './AdaptiveQueue';

// 1. 配置：阈值设置得比较敏感，方便在本地电脑快速看到效果
const manager = new AdaptiveQueue({
    minConcurrency: 1,
    maxConcurrency: 10,    // 允许最大开10个并发
    checkIntervalMs: 1000, // 每秒检查一次
    cooldownMs: 3000,      // 调整后冷却3秒
    thresholds: {
        memHigh: 0.60,     // 内存超过 60% 即视为危险 (本地电脑通常占用较高，设低点方便触发)
        cpuHigh: 0.50,     // CPU 超过 50% 即降级
        lagHigh: 20        // Lag 超过 20ms 即降级
    }
});

manager.startMonitoring();

// 2. 模拟重任务 (模拟浏览器实例)
const heavyTask = (id: number) => async () => {
    const start = Date.now();
    console.log(`Task ${id} started. Pending: ${manager.queue.pending}`);

    // 模拟内存消耗：分配 100MB 内存 (Buffer 是堆外内存，模拟 Chrome 进程消耗)
    // 注意：Node.js 有 GC，但 Buffer 在引用持有期间会占用物理内存
    const dummyMemory = Buffer.alloc(100 * 1024 * 1024); 

    // 模拟持续运行 5秒
    await new Promise<void>(resolve => {
        setTimeout(() => {
            // 访问一下 dummyMemory 防止被优化掉
            dummyMemory[0] = 1; 
            resolve();
        }, 5000);
    });

    const duration = Date.now() - start;
    console.log(`Task ${id} finished in ${duration}ms`);
};

// 3. 批量提交任务
async function run() {
    console.log('Injecting 50 heavy tasks...');
    for (let i = 1; i <= 50; i++) {
        manager.queue.add(heavyTask(i));
    }
}

run();
步骤 B: 执行验证与观察
在终端运行 ts-node verify.ts。

预期观察到的现象与指标解读：

阶段一：爬升 (Ramp Up)

日志：[ADJUST] Concurrency 1 -> 2 | SAFE ...

解释：初始任务少，内存和 CPU 均未触达阈值。队列开始尝试增加并发。

阶段二：触顶 (Threshold Hit)

操作：随着并发数达到 4 或 5，每个任务占用 100MB，总共占用了 400-500MB 内存。加上你电脑原本的应用（IDE, 浏览器），SystemMonitor 读到的 os.freemem() 急剧下降，计算出的 memRatio 超过 0.60 (60%)。

日志：[ADJUST] Concurrency 5 -> 2 | OVERLOAD (Mem: 62.1%, CPU: 15%, Lag: 2ms)

解释：监控生效。内存触发阈值，控制器强制将并发减半（或降至最低）。

阶段三：冷却与稳定 (Cooldown & Stabilize)

操作：并发降为 2 后，任务执行完成释放内存。但由于 cooldownMs: 3000 的存在，系统不会立即增加并发，而是等待 3 秒。

日志：3秒内无 ADJUST 日志。

解释：防止抖动生效。

阶段四：再次尝试 (Retry)

操作：内存恢复安全水位，控制器再次尝试 Concurrency 2 -> 3。

步骤 C: 验证失败的判定
如果出现以下情况，说明设计失败：

电脑卡死/OOM：说明 getSystemLoadRatio 或 getMemoryUsageRatio 获取不准确，或者检查间隔太长。

并发一直为 1：说明阈值设置过低（例如本地电脑常驻内存本来就占了 90%），导致无法利用资源。需要调整 memHigh。

并发疯狂跳变 (1 -> 10 -> 1)：说明冷却时间 (cooldownMs) 太短，没有给予系统足够的反应时间。

总结
此方案通过全局监控替代了无意义的进程监控，通过冷却时间解决了抖动问题。你可以在本地通过调整 verify.ts 中的 thresholds 参数，亲眼看到并发数随着你打开/关闭其他耗资源软件（如 Photoshop 或 Chrome 标签页）而自动升降，这就是最直观的验证。